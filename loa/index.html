<!DOCTYPE html>

<html lang="en">
<head>
    <title>My story on building a digital library aiming for the petabyte scale.</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel='stylesheet' id='googleFontbody-css' href='https://fonts.googleapis.com/css?family=PT+Serif:400,700'
          type='text/css' media='all'/>
    <link rel='stylesheet' id='googleFontHeading-css' href='https://fonts.googleapis.com/css?family=PT+Serif:400,700'
          type='text/css' media='all'/>
    <link rel='stylesheet' id='googleFontMenu-css' href='https://fonts.googleapis.com/css?family=PT+Serif:400,700'
          type='text/css' media='all'/>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/5.2.0/css/bootstrap.min.css"
          integrity="sha512-XWTTruHZEYJsxV3W/lSXG1n3Q39YIWOstqvmFsdNEEQfHoZ6vm6E9GK2OrF6DSJSpIbRbi+Nn0WDPID9O7xB2Q=="
          crossorigin="anonymous" referrerpolicy="no-referrer"/>

    <style>
        :root {
            --primary-bg-color: #F4F4F4;
            --secondary-bg-color: #FFFFFF;
            --title-color: #444444;
            --border-color: #e6e6e6;
        }

        body {
            background-color: var(--primary-bg-color);
            font-family: PT Serif, "Helvetica Neue", Arial, Helvetica, Verdana, sans-serif;
        }

        h1, h2, h3 {
            font-weight: 600;
            margin-bottom: 25px;
            margin-left: 70px;
            margin-right: 70px;
            color: var(--title-color);
        }

        h1 {
            font-size: 40px;
        }

        h2 {
            font-size: 30px;
        }

        h3 {
            font-size: 20px;
        }

        p {
            font-size: 18px;
            font-weight: 400;
            margin: 25px 100px 18px;
            line-height: 1.85em;
            color: #525452;
        }

        a {
            color: var(--title-color);
        }

        .container {
            background: var(--secondary-bg-color);
            border: 1px solid var(--border-color);
            border-top: none;
            max-width: 1080px;
            margin: auto;
        }
    </style>
</head>
<body>
<div class="container">
    <div class="mb-5">
        <img class="img-fluid w-100" style="margin-top: 12px;" alt="Thomas Cole - The Course of Empire, Destruction"
             src="assets/logo-background.png"/>
    </div>

    <h1>My story on building a digital library aiming for the petabyte scale.</h1>

    <div class="mb-5 mt-5">
        <hr class="w-50" style="margin-left: auto; margin-right: auto;">
    </div>

    <!-- TODO: TOC -->

    <h2>What is this page</h2>

    <p>
        This page describes how I created the <b><a
            href="https://github.com/bottomless-archive-project/library-of-alexandria">Library of Alexandria</a></b>
        project. The project consist of an application suite that can enable its users to crawl, index and search
        hundreds of millions of documents, creating a library on epic proportions using only commodity hardware.
    </p>

    <h2>The beginning</h2>
    <p>
        On a Friday night, after work, most people usually watch football, go to the gym or doing something useful with
        their life. Not everyone tough. I was an exception to this rule. As an introvert, I spent the last part of my
        day sitting in my room, reading an utterly boring sounding book called <a
            href="https://en.wikipedia.org/wiki/Epistulae_Morales_ad_Lucilium">"Moral letters to Lucilius"</a>. It was
        written by some <a href="https://en.wikipedia.org/wiki/Seneca_the_Younger">old dude</a> thousands of years ago.
        Definitely not the most fun sounding book fora Friday night. However, after reading it for about an hour, I
        realized that the title might be boring, but the contents are almost literally gold. Too bad there are not too
        many of these books survived the test of time.
    </p>
    <p>
        Realizing this unfortunate fact was my trigger event to start to work on this project. On that day, after a
        quick Google search I realized that only <a href="https://en.wikipedia.org/wiki/Lost_literary_work">less than 1%
        of ancient texts</a> survived to the modern day.
    </p>

    <h2>But how?</h2>
    <p>
        At this point I had a couple (more like a dozen) failed projects under my belt, so I was not too fond to start
        working on a new one. I had to motivate myself. After I set the target of saving as many documents as possible,
        I wanted to have a more tangible but quite hard to achieve goal. I set a 100 million books as my initial goal,
        and a billion books as my ultimate target. Ohh how naive I was.
    </p>
    <p>
        Next day, after waking up, I immediately started typing on my old and trustworthy PC. Because I have a very
        T-shaped knowledge in <b>Java</b>, the language of choice for this project was immediately determined. Also,
        because I like to create small prototypes to understand the problem I need to solve, I immediately started with
        one.
    </p>
    <p>
        The goal of the prototype was simple. I "just" wanted to download 10.000 documents to understand how hard it is
        to collect and kind of archive them. The immediate problem was that I didn't know where can I get links for this
        much amount of files that I need. Obviously, <a href="https://en.wikipedia.org/wiki/Site_map">sitemaps</a> can
        be useful in this scenario. However, there are a couple of reasons why it is not too useful in this case. Most
        of the time it doesn't contain the documents, or at least not all of them. Also, I would need to get a domain
        list to download the sitemaps for etc. The immediate thing that came into my mind that it is a lot of hassle and
        there must be an easier way. This is when the <a href="https://commoncrawl.org/">Common Crawl</a> project came
        into the view.
    </p>

    <h3>Common crawl</h3>

    <p>
        Common Crawl is a project that contains hundreds of terabytes of HTML source code from websites that were <a
            href="https://en.wikipedia.org/wiki/Web_crawler">crawled</a> by the project. They publish a new set of crawl
        data at the beginning of each month.
    </p>
    <p>
        It sounded exactly like the data that I needed. There was just one thing left to do. Grab the files and parse
        them with a HTML parser. This was the time when I realized that no matter what I do, it's not going to be an
        easy ride. When I downloaded the first entry provided by the Common Crawl project, I noticed that it was saved
        in a strange file format called <a href="https://en.wikipedia.org/wiki/Web_ARChive">WARC</a>.
    </p>
    <p>
        I found one Java library on <a href="https://github.com/Mixnode/mixnode-warcreader-java">Github</a> (thanks
        Mixnode) that was able to read these files. Unfortunately it was not maintained for the past couple of years. I
        picked it up and <a href="https://github.com/laxika/java-warc">forked it</a> to make it a little easier to use.
        (A couple of years later this repo was <a href="https://github.com/bottomless-archive-project/java-warc">moved
        under</a> the Bottomless Archive project as well.)
    </p>
    <p>
        Finally, at this point I was able to go through a bunch of webpages (parsing them in the process with <a
            href="https://jsoup.org/">JSoup</a>), grab all the links that contained pdf files based on the file
        extension then download them. Unsurprisingly, most of the pages (~60-80%) ended up being unavailable (<a
            href="https://en.wikipedia.org/wiki/HTTP_404#Soft_404">404 Not Found</a> and friends). After a quick
        cup of coffee I got the 10.000 documents on my hard drive. This is when I realized that I have one more problem
        to solve.
    </p>
    <p>
        So, when I started to view the documents, a lot of them simply failed to open. I had to look around for a
        library that could verify PDF documents. I had some experience with <a
            href="https://pdfbox.apache.org/">PDFBox</a> in the past, so it seemed to be a good go-to solution. It had
        no way to verify documents by default, but it could open and parse them and that was enough to filter out the
        incorrect ones. It felt a little-bit strange just to read the whole PDF into the memory to verify if it is
        correct or not, but hey I needed a simple fix for now and it worked really well.
    </p>
    <p>
        After doing a re-run, I came to the conclusion that 10.000 document can fit on around 1.5 GB of space. That's
        not too bad I thought. Let's crawl more because it sounds like a lot of fun. I left my PC there for about half
        an hour, just to test the app a bit more.
    </p>

    <h2>Okay, now what?</h2>

    <p>
        While running the test, I quickly realized that I get suboptimal download speed because the documents were
        processed on just one thread. It was time to parallelize our processes, but to do that first it was needed to
        synchronize the URL link generation and the downloading of documents. It makes no sense to generate 10.000 URLs
        per second in the memory while we can only download from 10 locations per second. We will just fill our memory
        with a bunch of URLs and get an OOM error pretty quickly. It was the time to split up our application and
        introduce a datastore that can act as an intermediate meeting place between the two applications. Let me
        introduce <a href="https://www.mysql.com/">MySQL</a> for you all.
    </p>
    <p>
        Was splitting up the application a good idea? Absolutely! What about introducing MySQL? You can make a guess
        right now. What do you think, how can MySQL handle a couple of hundred million strings in one table? Let me help
        you. It was a failure. But I didn't know that at the time, so let's proceed with the integration of the said
        database. After the app was split into two, and the generator application saved the URLs into a table (with a
        flag that can determine if the location was visited or not) the download application was able to visit them.
        Guess what? When I ran the whole app overnight, I got hundreds of thousands of documents saved next morning (my
        500 Mbps connection was super awesome back then).
    </p>

    <h3>Going elastic</h3>

    <p>
        Now I got a bunch of documents. The feeling was awesome! This was the point when I realized that the original
        archiving idea can be done on a grand scale. It was good to see a couple of hundred gigabytes of documents on my
        hard disk, but you know what would be better? Indexing them into a search engine, then having a way to search
        and view them.Initially I had little experience with indexing big datasets. I used <a
            href="https://solr.apache.org/">Solr</a> a while ago (like 6 years ago lol) so my initial idea came down to
        use that for the indexing. However, just by looking around for a bit before starting to work on the
        implementation I found <a href="https://www.elastic.co/what-is/elasticsearch">Elasticsearch</a>. It seemed to be
        superior over Solr in almost every way possible (except it was managed by a company but whatever). The major
        selling point was that it was easier to integrate with. As far as I know, bot of them are just a wrapper around
        Lucene so the performance should be fairly similar. Maybe once it will be worthwhile to rewrite the application
        suite to use pure Lucene without actually doing <a href="http://wiki.c2.com/?PrematureOptimization=">premature
        optimization</a>. However, until then, Elasticsearch is the name of the game.
    </p>
    <p>
        After figuring out how indexing can be done, I immediately started to work. Extended the downloader application
        with code that indexed the downloaded documents that passed verification, then deleted the existing dataset to
        free up space and started the whole downloading part yet again in the next night.
    </p>
    <p>
        The indexing worked remarkably well, so I started to work on a web frontend that could be used to search and
        view the documents. This was (controversially) called as the <i>Backend Application</i> in the beginning, then I
        quickly renamed it to the more meaningful name of <i>Web Application</i>. I'll use that name in this document to
        minimize the complexity.
    </p>

    <h3>Going angular</h3>

    <p>
        Initially the frontend code was written in <a href="https://github.com/angular/angular.js?">AngularJS</a>. Why?
        Why did I choose an obsolete technology to create the frontend of my next dream project? Because it was
        something I already understood quite well, I was familiar with and had a lot of experience in. At this stage I
        just wanted to progress with my proof of concept. Optimizations and cleanups can be done later. Also, I'm a
        backed guy, so the frontend code should be minimal right? Right?
    </p>
    <p>
        It started out as minimal, that's for sure. Also, because it only used dependencies that can be served by <a
            href="https://cdnjs.com/">cdnjs</a>, it was easy to build and integrate into a java app.
    </p>

    <!--
    finished the frontend
    tested, and it worked well
    had to add duplicate detection
    had to add compression
    -->
</div>
</body>
</html>