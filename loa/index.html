<!DOCTYPE html>

<html lang="en">
<head>
    <title>My story on building a digital library aiming for the petabyte scale.</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel='stylesheet' id='googleFontbody-css' href='https://fonts.googleapis.com/css?family=PT+Serif:400,700'
          type='text/css' media='all'/>
    <link rel='stylesheet' id='googleFontHeading-css' href='https://fonts.googleapis.com/css?family=PT+Serif:400,700'
          type='text/css' media='all'/>
    <link rel='stylesheet' id='googleFontMenu-css' href='https://fonts.googleapis.com/css?family=PT+Serif:400,700'
          type='text/css' media='all'/>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/5.2.0/css/bootstrap.min.css"
          integrity="sha512-XWTTruHZEYJsxV3W/lSXG1n3Q39YIWOstqvmFsdNEEQfHoZ6vm6E9GK2OrF6DSJSpIbRbi+Nn0WDPID9O7xB2Q=="
          crossorigin="anonymous" referrerpolicy="no-referrer"/>

    <style>
        :root {
            --primary-bg-color: #F4F4F4;
            --secondary-bg-color: #FFFFFF;
            --title-color: #444444;
            --border-color: #e6e6e6;
        }

        body {
            background-color: var(--primary-bg-color);
            font-family: PT Serif, "Helvetica Neue", Arial, Helvetica, Verdana, sans-serif;
        }

        h1, h2, h3 {
            font-weight: 600;
            margin-bottom: 25px;
            margin-left: 70px;
            margin-right: 70px;
            color: var(--title-color);
        }

        h1 {
            font-size: 40px;
        }

        h2 {
            font-size: 30px;
        }

        h3 {
            font-size: 20px;
        }

        p {
            font-size: 18px;
            font-weight: 400;
            margin: 25px 100px 18px;
            line-height: 1.85em;
            color: #525452;
        }

        a {
            color: var(--title-color);
        }

        .container {
            background: var(--secondary-bg-color);
            border: 1px solid var(--border-color);
            border-top: none;
            max-width: 1080px;
            margin: auto;
        }
    </style>
</head>
<body>
<div class="container">
    <div class="mb-5">
        <img class="img-fluid w-100" style="margin-top: 12px;" alt="Thomas Cole - The Course of Empire, Destruction"
             src="assets/logo-background.png"/>
    </div>

    <h1>My story on building a digital library aiming for the petabyte scale.</h1>

    <div class="mb-5 mt-5">
        <hr class="w-50" style="margin-left: auto; margin-right: auto;">
    </div>

    <!-- TODO: TOC -->

    <h2>What is this page</h2>

    <p>
        This page describes how I created the <b><a
            href="https://github.com/bottomless-archive-project/library-of-alexandria">Library of Alexandria</a></b>
        project. The project consist of an application suite that can enable its users to crawl, index and search
        hundreds of millions of documents, creating a library on epic proportions using only commodity hardware.
    </p>

    <h2>The beginning</h2>
    <p>
        On a Friday night, after work, most people usually watch football, go to the gym or doing something useful with
        their life. Not everyone tough. I was an exception to this rule. As an introvert, I spent the last part of my
        day sitting in my room, reading an utterly boring sounding book called <a
            href="https://en.wikipedia.org/wiki/Epistulae_Morales_ad_Lucilium">"Moral letters to Lucilius"</a>. It was
        written by some <a href="https://en.wikipedia.org/wiki/Seneca_the_Younger">old dude</a> thousands of years ago.
        Definitely not the most fun sounding book fora Friday night. However, after reading it for about an hour, I
        realized that the title might be boring, but the contents are almost literally gold. Too bad there are not too
        many of these books survived the test of time.
    </p>
    <p>
        Realizing this unfortunate fact was my trigger event to start to work on this project. On that day, after a
        quick Google search I realized that only <a href="https://en.wikipedia.org/wiki/Lost_literary_work">less than 1%
        of ancient texts</a> survived to the modern day.
    </p>

    <h2>But how?</h2>
    <p>
        At this point I had a couple (more like a dozen) failed projects under my belt, so I was not too fond to start
        working on a new one. I had to motivate myself. After I set the target of saving as many documents as possible,
        I wanted to have a more tangible but quite hard to achieve goal. I set a 100 million books as my initial goal,
        and a billion books as my ultimate target. Ohh how naive I was.
    </p>
    <p>
        Next day, after waking up, I immediately started typing on my old and trustworthy PC. Because I have a very
        T-shaped knowledge in <b>Java</b>, the language of choice for this project was immediately determined. Also,
        because I like to create small prototypes to understand the problem I need to solve, I immediately started with
        one.
    </p>
    <p>
        The goal of the prototype was simple. I "just" wanted to download 10.000 documents to understand how hard it is
        to collect and kind of archive them. The immediate problem was that I didn't know where can I get links for this
        much amount of files that I need. Obviously, <a href="https://en.wikipedia.org/wiki/Site_map">sitemaps</a> can
        be useful in this scenario. However, there are a couple of reasons why it is not too useful in this case. Most
        of the time it doesn't contain the documents, or at least not all of them. Also, I would need to get a domain
        list to download the sitemaps for etc. The immediate thing that came into my mind that it is a lot of hassle and
        there must be an easier way. This is when the <a href="https://commoncrawl.org/">Common Crawl</a> project came
        into the view.
    </p>

    <h3>Common crawl</h3>

    <p>
        Common Crawl is a project that contains hundreds of terabytes of HTML source code from websites that were <a
            href="https://en.wikipedia.org/wiki/Web_crawler">crawled</a> by the project. They publish a new set of crawl
        data at the beginning of each month.
    </p>
    <p>
        It sounded exactly like the data that I needed. There was just one thing left to do. Grab the files and parse
        them with a HTML parser. This was the time when I realized that no matter what I do, it's not going to be an
        easy ride. When I downloaded the first entry provided by the Common Crawl project, I noticed that it was saved
        in a strange file format called <a href="https://en.wikipedia.org/wiki/Web_ARChive">WARC</a>.
    </p>
    <p>
        I found one Java library on <a href="https://github.com/Mixnode/mixnode-warcreader-java">Github</a> (thanks
        Mixnode) that was able to read these files. Unfortunately it was not maintained for the past couple of years. I
        picked it up and <a href="https://github.com/laxika/java-warc">forked it</a> to make it a little easier to use.
        (A couple of years later this repo was <a href="https://github.com/bottomless-archive-project/java-warc">moved
        under</a> the Bottomless Archive project as well.)
    </p>
    <p>
        Finally, at this point I was able to go through a bunch of webpages (parsing them in the process with <a
            href="https://jsoup.org/">JSoup</a>), grab all the links that contained pdf files based on the file
        extension then download them. Unsurprisingly, most of the pages (~60-80%) ended up being unavailable (<a
            href="https://en.wikipedia.org/wiki/HTTP_404#Soft_404">404 Not Found</a> and friends). After a quick
        cup of coffee I got the 10.000 documents on my hard drive. This is when I realized that I have one more problem
        to solve.
    </p>

    <h3>Unboxing & validation</h3>

    <p>
        So, when I started to view the documents, a lot of them simply failed to open. I had to look around for a
        library that could verify PDF documents. I had some experience with <a
            href="https://pdfbox.apache.org/">PDFBox</a> in the past, so it seemed to be a good go-to solution. It had
        no way to verify documents by default, but it could open and parse them and that was enough to filter out the
        incorrect ones. It felt a little-bit strange just to read the whole PDF into the memory to verify if it is
        correct or not, but hey I needed a simple fix for now and it worked really well.
    </p>
    <p>
        After doing a re-run, I came to the conclusion that 10.000 document can fit on around 1.5 GB of space. That's
        not too bad I thought. Let's crawl more because it sounds like a lot of fun. I left my PC there for about half
        an hour, just to test the app a bit more.
    </p>

    <h2>Okay, now what?</h2>

    <p>
        While running the test, I quickly realized that I get suboptimal download speed because the documents were
        processed on just one thread. It was time to parallelize our processes, but to do that first it was needed to
        synchronize the URL link generation and the downloading of documents. It makes no sense to generate 10.000 URLs
        per second in the memory while we can only download from 10 locations per second. We will just fill our memory
        with a bunch of URLs and get an OOM error pretty quickly. It was the time to split up our application and
        introduce a datastore that can act as an intermediate meeting place between the two applications. Let me
        introduce <a href="https://www.mysql.com/">MySQL</a> for you all.
    </p>
    <p>
        Was splitting up the application a good idea? Absolutely! What about introducing MySQL? You can make a guess
        right now. What do you think, how can MySQL handle a couple of hundred million strings in one table? Let me help
        you. It was a failure. But I didn't know that at the time, so let's proceed with the integration of the said
        database. After the app was split into two, and the generator application saved the URLs into a table (with a
        flag that can determine if the location was visited or not) the download application was able to visit them.
        Guess what? When I ran the whole app overnight, I got hundreds of thousands of documents saved next morning (my
        500 Mbps connection was super awesome back then).
    </p>

    <h3>Going elastic</h3>

    <p>
        Now I got a bunch of documents. The feeling was awesome! This was the point when I realized that the original
        archiving idea can be done on a grand scale. It was good to see a couple of hundred gigabytes of documents on my
        hard disk, but you know what would be better? Indexing them into a search engine, then having a way to search
        and view them.Initially I had little experience with indexing big datasets. I used <a
            href="https://solr.apache.org/">Solr</a> a while ago (like 6 years ago lol) so my initial idea came down to
        use that for the indexing. However, just by looking around for a bit before starting to work on the
        implementation I found <a href="https://www.elastic.co/what-is/elasticsearch">Elasticsearch</a>. It seemed to be
        superior over Solr in almost every way possible (except it was managed by a company but whatever). The major
        selling point was that it was easier to integrate with. As far as I know, bot of them are just a wrapper around
        Lucene so the performance should be fairly similar. Maybe once it will be worthwhile to rewrite the application
        suite to use pure Lucene without actually doing <a href="http://wiki.c2.com/?PrematureOptimization=">premature
        optimization</a>. However, until then, Elasticsearch is the name of the game.
    </p>
    <p>
        After figuring out how indexing can be done, I immediately started to work. Extended the downloader application
        with code that indexed the downloaded documents that passed verification, then deleted the existing dataset to
        free up space and started the whole downloading part yet again in the next night.
    </p>
    <p>
        The indexing worked remarkably well, so I started to work on a web frontend that could be used to search and
        view the documents. This was (controversially) called as the <i>Backend Application</i> in the beginning, then I
        quickly renamed it to the more meaningful name of <i>Web Application</i>. I'll use that name in this document to
        minimize the complexity.
    </p>

    <h3>Going angular</h3>

    <p>
        Initially the frontend code was written in <a href="https://github.com/angular/angular.js?">AngularJS</a>. Why?
        Why did I choose an obsolete technology to create the frontend of my next dream project? Because it was
        something I already understood quite well, I was familiar with and had a lot of experience in. At this stage I
        just wanted to progress with my proof of concept. Optimizations and cleanups can be done later. Also, I'm a
        backed guy, so the frontend code should be minimal right? Right?
    </p>
    <p>
        It started out as minimal, that's for sure. Also, because it only used dependencies that can be served by <a
            href="https://cdnjs.com/">cdnjs</a>, it was easy to build and integrate into a java app.
    </p>
    <div class="float-start" style="margin-left: 100px; margin-right: 18px; margin-top: 18px;">
        <div>
            <img class="rounded float-start" src="assets/images/xanthoria-parietina.jpg" alt="Xanthoria parietina">
        </div>
        <div class="text-center">
            <div>
                <small>Xanthoria parietina</small>
            </div>
            <div>
                <small>
                    <a href="https://commons.wikimedia.org/wiki/User:Holleday">&copy; Holger Krisp</a> - <a
                        href="https://creativecommons.org/licenses/by/3.0">CC BY 3.0</a>
                </small>
            </div>
        </div>
    </div>
    <p>
        Soon the frontend was finished and I had some time to actually search and read the documents I collected. I
        remember that I wanted to search something obscure. I was studying gardening back then, so my first search was
        for the lichen <b>"Xanthoria parietina"</b>. To my surprise, I got back around a hundred documents from a 2.3
        million
        sample size. Honestly, I was surprised. Some of them were quite interesting. Like whom wouldn't want to read
        the <i>"Detection of polysaccharides and ultrastructural modification of the photobiont cell wall produced by
        two arginase isolectins from Xanthoria parietina"</i>?
    </p>

    <h2>Saving space</h2>

    <p>
        Soon after I got the first couple of million documents I realized that I'll need some space to store them. A LOT
        of space, actually. Because of this realization, I started to look for ideas that could save me as much space as
        possible, so I can store as many documents as possible.
    </p>

    <h3>Deduplication</h3>

    <p>
        While searching using the web UI, I found a couple of duplicates in the dataset. This was not too problematic in
        the beginning, but more documents the program downloaded, the more duplicates I saw on the frontend. I had to do
        something.
    </p>

    <p>
        My first idea was to use a <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filter</a>. Initially it
        felt like a good idea. I initialized a filter with an expectation of 50 million items and a 1% false positive
        probability. Like what a fool would collect more than 50 million documents? Guess what, I ended up throwing the
        whole thing into the garbage bin after hitting 5 million files in a couple of weeks. Who would want to
        re-size the Bloom filter every time after a new max value is hit? Also, 1% of false positives felt way too high.
    </p>

    <p>
        The next try was to calculate file <a href="https://en.wikipedia.org/wiki/Checksum">checksums</a>. A checksum is
        useful to verify file integrity after long time of storage, but it can also be used to detect duplicates. I
        started with <a href="https://en.wikipedia.org/wiki/MD5">MD5</a> as a hash function to generate the checksums.
        It is well known that albeit MD5 is super quick, it is broken for password hashing. Still I thought that it can
        work for files nevertheless. Unfortunately, there is a thing that's called <a
            href="https://en.wikipedia.org/wiki/Hash_collision">hash-collision</a>.
    </p>

    <p>
        After learning that MD5 can have collisions, especially if we take the <a
            href="https://en.wikipedia.org/wiki/Birthday_problem">Birthday-problem</a> into consideration, I wanted
        something better. This is when I realized that with using <a
            href="https://en.wikipedia.org/wiki/SHA-2">SHA-256</a>, the chance of a collision was significantly lower.
        Luckily, my code was quite well abstracted, so it was easy to replace the MD5 generation with SHA-256. In the
        final duplicate detection algorithm, a document can be considered a duplicate if it's <b>file type
        (extension)</b>, <b>file size</b>, and <b>checksum</b> is the same. After implementing the change, I had to
        re-crawl all the documents, but finally without duplication.
    </p>

    <div class="float-start">
        <div>
            <img class="img-fluid" src="assets/images/hash-collision-probabilities.png"
                 alt="Hash collision probabilities" style="padding-left: 100px; padding-right: 100px;">
        </div>
        <div class="text-center">
            <div style="margin-left: 100px; margin-right: 100px;">
                <small>
                    The lighter fields in this table show the number of hashes needed to achieve the given probability
                    of collision (column) given a hash space of a certain size in bits (row).
                </small>
            </div>
            <div style="margin-left: 100px; margin-right: 100px;">
                <small>
                    Using the birthday analogy: the "hash space size" resembles the "available days", the "probability
                    of collision" resembles the "probability of shared birthday", and the "required number of hashed
                    elements" resembles the "required number of people in a group".
                </small>
            </div>
            <div>
                <small>
                    &copy; <a href="https://en.wikipedia.org/wiki/Birthday_problem">Wikipedia</a> -
                    <a href="https://creativecommons.org/licenses/by-sa/3.0/deed.en_US">CC BY-SA 3.0</a>
                </small>
            </div>
        </div>
    </div>

    <h3>Compression</h3>

    <p>
        Removing duplicates saved a lot of space, but still, I kept acquiring more documents than what I had space for.
        This made me really desperate to lower the average document size, so I came up with an easy idea to cram more
        documents to the same space. I planned to compress them.
    </p>

    <p>
        There are a couple of ways to compress a PDF document. Or rather a couple of <a
            href="https://en.wikipedia.org/wiki/Lossless_compression">lossless</a> compression algorithms to be correct.
    </p>

    <p>
        The first thing I looked into was the good old <a href="https://en.wikipedia.org/wiki/Deflate">Deflate</a>
        algorithm with <a href="https://en.wikipedia.org/wiki/Gzip">GZIP</a> as the file format. It had certain
        advantages. First of all, it was very <b>mature</b> supported by almost anything, including Java (albeit later
        on I switched to the <a href="https://commons.apache.org/proper/commons-compress/">Apache Compress</a> library
        for usability reason). Secondly, it was very fast and had an <i>"okay"</i> compression ration.
    </p>

    <p>
        GZIP was good enough for most of the time, but when I had spare CPU cycles to use, I wanted to re-compress the
        documents with something that had a better compression ratio. This was the time when I found out about the <a
            href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Markov_chain_algorithm">LZMA</a> encoding.
        Unlike GZIP (which uses a combination of <a href="https://en.wikipedia.org/wiki/LZ77_and_LZ78#LZ77">Z77</a> and
        <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman coding</a>), it does <a
            href="https://en.wikipedia.org/wiki/Dictionary_coder">dictionary based</a> compression. It is a <b>mostly
        mature</b> algorithm too, that has excellent compression ratio, good decompression speed but abysmal compression
        speed. Ideal for long term archival, especially when paired with an extensive amount of free CPU resources.
    </p>

    <p>
        The final candidate for compression was <a href="https://en.wikipedia.org/wiki/Brotli">Brotli</a>, a relative
        new algorithm that was originally intended to replace deflate on the world wide web. It is mainly used by <a
            href="https://en.wikipedia.org/wiki/Web_server">web servers</a> and <a
            href="https://en.wikipedia.org/wiki/Content_delivery_network">content delivery networks</a> to serve web
        data. Unfortunately, I found just one library that supported it in Java (<a
            href="https://github.com/hyperxpro/Brotli4j">Brotli4J</a>) and even that one was not a real Java rewrite but
        a wrapper around the <a href="https://github.com/google/brotli">native library</a> provided by Google. It's
        feels <b>very immature</b>, mostly because it was released in 2015 (unlike Deflate which was released in 1951
        and LZMA in 1998). But, it provides the best compression ratio out of the tree by far. Unfortunately, the
        resource usage is very high as well, it is the slowest one on the list. ALso, to function, it requires a native
        port for each and every operation system. A hustle to deal with.
    </p>

    <h3>Welcome MongoDB</h3>

    <p>
        As described earlier, <a href="https://www.mysql.com/">MySQL</a> was good for a while. For like the first 5-10
        million documents. Then it started to slow down "significantly" to say the least. Something had to be done,
        especially because back then my aim was to archive a billion documents. I kept on looking for databases, then
        realized that what I need is probably a <a href="https://www.geeksforgeeks.org/introduction-to-nosql/">NoSQL</a>
        database, or something similar. All my database had to do was return me all the data for either a document's or
        a location's id, support <a href="https://www.mongodb.com/docs/manual/core/index-unique/">unique indexes</a> on
        certain fields and be able to scale when necessary. My usage pattern was inserting document locations
        with a hash as the id, or finding documents with combinations of certain fields (document type, file size and
        hash for deduplication).
    </p>
    <p>
        My first idea was to use <a href="https://redis.io/">Redis</a> for super quick access. How cool is the idea to
        store everything my database might need in the ram. Until I realized that to do this I would need to use some
        really awkward primary keys ({documentType}_{documentFileSite}_{documentHash}) to save documents and also my
        database can get big, really big where even the max memory size available with DDR-4 RAM might not be
        enough (128 GB).
    </p>
    <p>
        Then I looked into using <a href="https://www.mongodb.com/">MongoDB</a>. It looked like exactly what I need and
        because I designed a small but useful abstraction layer over my MySQL code, replacing the existing database was
        close to trivial.
    </p>
    <p>
        As usual, there were a few hiccups that I realized later on. First of all, a couple of weeks later I added a
        statistics screen that printed not only the number of documents that were collected, but the count of each
        document type (pdf, doc, etc.) and also the count of each document status (how many are indexed, downloaded,
        etc.). I thought that this will be easy. A simple select (find in MongoDB terms) combined with a group by then
        returning the values to the UI. Unfortunately, it is not that easy. MongoDB does aggregations, and those can
        only run on one core, so even if my "type" and status" fields are indexed, it takes a significant amount of time
        for one CPU core to sum up a 100 million documents. As far as I know, there is no way to really speed this up. I
        could do a select and count for each document type (i.e. 8 queries instead of one with a groupby) but that would
        be only a band-aid. MongoDB should add some kind of partitioning for these queries, otherwise, I'll ned to wait
        half an hour to get a result when I'll have 1 billion documents (yes, that's still my long term target).
    </p>
</div>
</body>
</html>
